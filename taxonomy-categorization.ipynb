{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330d4f1c-7e0d-4799-9664-ac9b35ffa342",
   "metadata": {},
   "source": [
    "# Taxonomy Categorization Use-Case\n",
    "\n",
    "This notebook demonstrates a taxonomy categorization use-case using text embeddings and Vertex AI Vector Search.\n",
    "\n",
    "### The steps performed include:\n",
    "* Parameters, variables, and any helper functions are defined\n",
    "* Sample pre-processing to deduplicate initial data; Combine and transform relevant data columns\n",
    "* Apply text embeddings using REST requests\n",
    "* Create and Deploy Index for Vector Search\n",
    "* Query Index and get Ranked Results of classes\n",
    "\n",
    "User paramaters are indicated by `@param`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027985a0-f499-4862-953c-981886b73fac",
   "metadata": {},
   "source": [
    "### Import Libraries & Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc870ec-f03a-47d1-80e5-25991994c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the packages\n",
    "# ! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "#                                  google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf7ff3-c38b-422e-8690-d96ea0f445f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import google.auth.transport.requests\n",
    "import google.auth\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import \\\n",
    "    Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420b831-e725-49c0-b3c1-d48fa999f79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-401718\" # @param\n",
    "REGION = \"us-central1\" # @param\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-category-textembedding-{REGION}\"\n",
    "INPUT_URI = f\"{BUCKET_URI}/input-test\"\n",
    "OUTPUT_URI = f\"{BUCKET_URI}/output-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895728e-5961-4d93-a6fe-65cc6f295288",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d63b91-2393-487a-9b04-252b1e1888c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be594344-bb82-47dc-b8b4-892ba437762e",
   "metadata": {},
   "source": [
    "## Textembeddings on GCS\n",
    "\n",
    "The following data splitting strategy is used for building and querying the index:\n",
    "\n",
    "![taxonomy-flow_.png](./imgs/taxonomy-flow_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f915a0b-0b1f-46f1-bc53-912fc9459014",
   "metadata": {},
   "source": [
    "### Pre-processing and Enrichment\n",
    "\n",
    "Preprocess involves cleaning, transforming, and standardizing the raw data to reduce noise and inconsistencies. Preprocessing can help ensure optimal performance and accurate results for downstream matching algorithms.\n",
    "\n",
    "Sample preprocessing techniques to consider:\n",
    "* Remove exact duplicate records (included below) \n",
    "* Normalize keyword variations in spelling and abbreviations (e.g., \"Ave\" to \"Avenue\")\n",
    "* Ensure consistent formatting across all data points\n",
    "* Identify and correct invalid or inaccurate domain information\n",
    "\n",
    "\n",
    " Information is also strategically combined from multiple sources to create richer representations of product taxonomy classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693214a-49b5-4959-8006-eb4816788c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(' AAG_test_dataset.xlsx - Descriptions.csv')\n",
    "df_heirarchy = pd.read_csv('EUHIERARCHY_062024.xlsx - Sheet1.csv')\n",
    "df_data_holdout = pd.read_csv('inventory-test-source.csv') # used for testing accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158def71-e040-490d-aba0-6edec5822c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split unlabeled and labeled test data set\n",
    "\n",
    "# DataFrame where 'structure_assignment_euhierarchy' is NOT null\n",
    "df_data_label = df_data[df_data['structure_assignment_euhierarchy'].notna()].copy() \n",
    "\n",
    "# Remove holdout dataset from Labeled dataset \n",
    "# Perform the merge with indicator\n",
    "merged_df = df_data_label.merge(df_data_holdout, how='left', indicator=True)\n",
    "df_data_label = merged_df[merged_df['_merge'] == 'left_only'].drop('_merge', axis=1)\n",
    "\n",
    "# DataFrame where 'structure_assignment_euhierarchy' IS null\n",
    "df_data_unlabel = df_data[df_data['structure_assignment_euhierarchy'].isnull()].copy() # simulate live data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee211b2-efa2-4d42-b9da-43eb50c51d24",
   "metadata": {},
   "source": [
    "### Combine and transform relevant data columns\n",
    "\n",
    "Engineer data to be optimized for embedding generation. For example, merging a dataset containing multiple product examples per category with the hierarchical taxonomy dataset provides the embedding model with a more diverse and comprehensive understanding of each category. Instead of relying solely on the taxonomy, which only offers one example per class, this method leverages labeled data to enhance the model to learn more nuanced representations, ultimately enhancing categorization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1e510-a584-47ab-86ac-f98469ad89b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the dataframes based on the 'structure_assignment_euhierarchy' and 'Tier_5_ID' columns\n",
    "\n",
    "## This provides additional \"ground truth\" examples for the vector database instead \n",
    "## of just relying on the heirarchy which only includes 1 example per unique class\n",
    "\n",
    "df_data_merge = pd.merge(\n",
    "    df_data_label[\n",
    "        [\n",
    "            \"supplier\",\n",
    "            \"short_description_en\",\n",
    "            \"supplier_description_en\",\n",
    "            \"long_description_en\",\n",
    "            \"structure_assignment_euhierarchy\",\n",
    "        ]\n",
    "    ],\n",
    "    df_heirarchy[\n",
    "        [\n",
    "            \"Tier_1_EN\",\n",
    "            \"Tier_2_EN\",\n",
    "            \"Tier_3_EN\",\n",
    "            \"Tier_4_EN\",\n",
    "            \"Tier_5_EN\",\n",
    "            \"Tier_5_ID\",\n",
    "        ]\n",
    "    ],\n",
    "    left_on=\"structure_assignment_euhierarchy\",\n",
    "    right_on=\"Tier_5_ID\",\n",
    "    how=\"right\",\n",
    ")\n",
    "\n",
    "# Label data with unique ID's to map embeddings back to class\n",
    "np.random.seed(42)\n",
    "df_data_merge['unique_id'] = np.random.randint(10000000000000, size=len(df_data_merge))\n",
    "if df_data_merge['unique_id'].nunique() == len(df_data_merge):\n",
    "    print(\"All IDs are unique\")\n",
    "else:\n",
    "    print(\"There are duplicate IDs!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c29e3-c7d7-4fd0-ba1f-e9295616af2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format features for embeddings\n",
    "\n",
    "# Select and combine features into comma separated string\n",
    "\n",
    "df_data_merge['combined_text'] = (df_data_merge['supplier'].fillna('') + ', ' + \n",
    "                       df_data_merge['short_description_en'].fillna('') + ', ' + \n",
    "                       df_data_merge['supplier_description_en'].fillna('') + ', ' + \n",
    "                       df_data_merge['long_description_en'].fillna('') + ', ' +\n",
    "                       df_data_merge['Tier_1_EN'].fillna('') + ', ' +  \n",
    "                       df_data_merge['Tier_2_EN'].fillna('') + ', ' + \n",
    "                       df_data_merge['Tier_3_EN'].fillna('') + ', ' + \n",
    "                       df_data_merge['Tier_4_EN'].fillna('') + ', ' + \n",
    "                       df_data_merge['Tier_5_EN'].fillna('') + ', ' + \n",
    "                       df_data_merge['Tier_5_ID'].fillna(''))\n",
    "\n",
    "df = df_data_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f3e27-5973-43bb-93c2-86f7e49d3cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['combined_text'][13000] # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6e423-9db2-4c1a-8a08-e855cd25f01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create JSON file\n",
    "# Create a list of dictionaries from the 'combined_text' column\n",
    "text_data = [{\"content\": combined_text} for combined_text in df['combined_text']]   ########\n",
    "# text_data = [{\"content\": combined_text_sentence} for combined_text_sentence in df['combined_text_sentence']] #######\n",
    "\n",
    "# Save to a JSONL file\n",
    "with open('input.jsonl', 'w') as outfile:\n",
    "    for entry in text_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72532a34-93f6-425f-8ab7-4abd60144567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to GCS \n",
    "storage_client = storage.Client()\n",
    "\n",
    "BUCKET_NAME = \"/\".join(INPUT_URI.split(\"/\")[:3])\n",
    "bucket = storage_client.bucket(BUCKET_NAME[5:])\n",
    "\n",
    "# Define the blob including any folders from INPUT_URI\n",
    "blob_name = \"/\".join(INPUT_URI.split(\"/\")[3:])+\"/input.jsonl\"\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Upload the file \n",
    "blob.upload_from_filename(\"input.jsonl\")\n",
    "\n",
    "print(f\"File uploaded to cloud storage in {INPUT_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820b5e2-d866-4500-b0ee-5b8668bfefe7",
   "metadata": {},
   "source": [
    "### REST request for Batch Prediction Job\n",
    "\n",
    "This section details the code that makes the API request to Vertex AI to generate the text embeddings.\n",
    "<br> For list of available embeddings models see: https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b742112-7297-47f8-9b02-58119ec4d6ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "\n",
    "# Set up Application Default Credentials (ADC)\n",
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e1a7e-a8d5-4db4-8218-40592fbf7970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"publishers/google/models/text-embedding-004\" # optimized for english content\n",
    "# MODEL = \"publishers/google/models/text-multilingual-embedding-002\"\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/us-central1/batchPredictionJobs\"\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2d587-a728-43ed-880a-0fa5b57074e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = str(\n",
    "    {\n",
    "        \"name\": \"batch-test\",\n",
    "        \"displayName\": \"batch-test\",\n",
    "        \"model\": MODEL,\n",
    "        \"inputConfig\": {\n",
    "            \"instancesFormat\": \"jsonl\",\n",
    "            \"gcs_source\": {\"uris\": [f\"{INPUT_URI}/input.jsonl\"]},\n",
    "        },\n",
    "        \"outputConfig\": {\n",
    "            \"predictionsFormat\": \"jsonl\",\n",
    "            \"gcs_destination\": {\"output_uri_prefix\": OUTPUT_URI},\n",
    "        },\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4769e5c-eff4-4d47-89fb-68c89ff6e15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# request_body = '{\"name\": \"test\", \"displayName\": \"test\", \"model\": \"publishers/google/models/text-embedding-004\", \"inputConfig\": {\"instancesFormat\": \"jsonl\", \"gcs_source\": {\"uris\": [\"gs://sandbox-401718-fuzzymatch-textembedding/input-test/input.jsonl\"]}}, \"outputConfig\": {\"predictionsFormat\": \"jsonl\", \"gcs_destination\": {\"output_uri_prefix\": \"gs://sandbox-401718-fuzzymatch-textembedding/output-test\"}}}'\n",
    "r = requests.post(url, data=request_body, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd4f2e-6035-48a2-869f-59546f16907b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44333e-2fc7-4981-a7b2-08be56d2b145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb13b1-df17-4466-ab31-d6d93c7e9c7f",
   "metadata": {},
   "source": [
    "### Import JSON From Path and Map\n",
    "\n",
    "This section outlines the steps of retrieving the generated embeddings files from the Batch Job output and creating a mapping to tie it back to the respective accounts in the original data. <br> **Note** To keep things simple, users need to provide the paths to the output embedding files manually. In the future, this process can be automated by retrieving the file paths directly from the Batch Prediction Job's output information.\n",
    "\n",
    "* Download embed file: The embedding results from the batch prediction job are downloaded from GCS.\n",
    "* Map embeddings to original dataset: The downloaded embeddings are associated with their corresponding addresses in the original DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee491e-b8c6-46cc-bcc3-024ba73ed308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBED_FILES = [\n",
    "    \"gs://sandbox-401718-category-textembedding-us-central1/output-test/prediction-model-2024-08-02T18:50:02.710567Z/000000000000.jsonl\",  # @param\n",
    "]\n",
    "\n",
    "# Write to the local JSON Lines file directly\n",
    "with open(\"embeddings.jsonl\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for embed_file in EMBED_FILES:\n",
    "        bucket_name = embed_file.split(\"/\")[2]\n",
    "        blob_name = \"/\".join(embed_file.split(\"/\")[3:])\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Download the entire file as a string\n",
    "\n",
    "        file_content = blob.download_as_string().decode(\"utf-8\")\n",
    "        lines = file_content.splitlines()\n",
    "        for line in lines:\n",
    "            outfile.write(line + \"\\n\")  # Write each line as a separate JSON object\n",
    "print(f\"Combined JSON Lines data saved to `embeddings.jsonl`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd215696-ff59-4792-a762-705245cd8e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the JSON data from your local .jsonl file\n",
    "response_json = []\n",
    "with open(\"embeddings.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        response_json.append(json.loads(line))  # Parse each line\n",
    "\n",
    "# Create a dictionary to map addresses to embeddings\n",
    "combined_text_embedding_map = {}\n",
    "for item in response_json:\n",
    "    combined_text = item['instance']['content']\n",
    "    embedding = item['predictions'][0]['embeddings']['values'] \n",
    "    combined_text_embedding_map[combined_text] = embedding\n",
    "\n",
    "# Map embeddings to the DataFrame using the address lookup\n",
    "df['Embeddings'] = df['combined_text'].map(combined_text_embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a16454-6931-4957-a11a-e52178decc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head() # Embeddings column now included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af100bfb-e053-4ac2-9a82-144749934e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # check unique id's\n",
    "# combined_text_ = df['unique_id'].unique()\n",
    "# len(combined_text_) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbf52b-0c72-4d92-b202-66f09b79755b",
   "metadata": {},
   "source": [
    "## Create Index for Vector Search\n",
    "\n",
    "This section describes the process of creating an index for Vertex Vector Search. Bruce force (exhaustive) search index is used in this example, and is used to find the exact nearest neighbors to the query vector. Brute force Index is computationally rigorous compared to ANN which is focuses on performant approximations and retrieval efficiency.\n",
    "\n",
    "For more information about the methods and their tradeoff: https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778cee7-8628-4e44-9386-c0725e6d3bcc",
   "metadata": {},
   "source": [
    "### Format data\n",
    "https://cloud.google.com/vertex-ai/docs/vector-search/setup/format-structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08445f-976c-4c1f-a8d7-70c64e11e279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of dictionaries (same as before)\n",
    "data = []\n",
    "for index, row in df.iterrows(): \n",
    "    data.append({\n",
    "        \"id\": str(row['unique_id']),\n",
    "        \"embedding\": row['Embeddings'],\n",
    "        # \"address_hash_key\": row['address_hash_key']\n",
    "    })\n",
    "    \n",
    "# Export the data as a JSON Lines file\n",
    "with open(\"index_input_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data:\n",
    "        json.dump(entry, f)  # Write each dictionary as JSON\n",
    "        f.write('\\n')        # Add a newline to separate objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128b53c-c2da-4deb-8f0a-f813263d7bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to GCS \n",
    "BUCKET_NAME = \"/\".join(INPUT_URI.split(\"/\")[:3])\n",
    "bucket = storage_client.bucket(BUCKET_NAME[5:])\n",
    "\n",
    "# Define the blob including any folders from INPUT_URI\n",
    "blob_name = \"/\".join(INPUT_URI.split(\"/\")[3:])+\"/initial/index_input_data.json\"\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "# Upload the file \n",
    "blob.upload_from_filename(\"index_input_data.json\")\n",
    "\n",
    "print(f\"File uploaded to cloud storage in {INPUT_URI}/initial/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5f620-b8ee-49fb-b264-1493e7568b1f",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "For similarity calculations, the documentation strongly recommends using DOT_PRODUCT_DISTANCE + UNIT_L2_NORM instead of the COSINE distance. These algorithms have been more optimized for the DOT_PRODUCT distance, and when combined with UNIT_L2_NORM, offers the same ranking and mathematical equivalence as the COSINE distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47893c22-220e-45c5-8b33-200bb1cd7d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=INPUT_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90dec7-df63-4cff-a14d-16acac50ee24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = len(df[\"Embeddings\"][0])\n",
    "DISPLAY_NAME = \"index_category\"\n",
    "DISPLAY_NAME_BRUTE_FORCE = DISPLAY_NAME + \"_brute_force\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91986364-3ccd-40d4-b985-a6c4c3b0979b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brute_force_index = aiplatform.MatchingEngineIndex.create_brute_force_index(\n",
    "    display_name=DISPLAY_NAME_BRUTE_FORCE,\n",
    "    contents_delta_uri=f\"{INPUT_URI}/initial/\",\n",
    "    dimensions=DIMENSIONS,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    feature_norm_type=\"UNIT_L2_NORM\",\n",
    "    description=\"Category index (brute force)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bc6e6-3e09-478a-b582-d2d36c5f14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_BRUTE_FORCE_RESOURCE_NAME = brute_force_index.resource_name #'projects/757654702990/locations/us-central1/indexes/9080369554546229248'\n",
    "\n",
    "brute_force_index = aiplatform.MatchingEngineIndex(\n",
    "    index_name=INDEX_BRUTE_FORCE_RESOURCE_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e9bda-a2f5-46fc-af50-8481d982ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "#     display_name=DISPLAY_NAME,\n",
    "#     contents_delta_uri=EMBEDDINGS_INITIAL_URI,\n",
    "#     dimensions=DIMENSIONS,\n",
    "#     approximate_neighbors_count=150,\n",
    "#     distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "#     leaf_node_embedding_count=500,\n",
    "#     leaf_nodes_to_search_percent=7,\n",
    "#     description=\"ANN index\",\n",
    "#     labels={\"label_name\": \"label_value\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dee79-d066-4052-8fb7-bc4393ccf8ca",
   "metadata": {},
   "source": [
    "### Deploy Index to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd4be8-f2ed-40b3-ac8d-7f31de6d1747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the project number\n",
    "PROJECT_NUMBER = !gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = PROJECT_NUMBER[0]\n",
    "\n",
    "VPC_NETWORK = \"beusebio-network\"\n",
    "VPC_NETWORK_FULL = f\"projects/{PROJECT_NUMBER}/global/networks/{VPC_NETWORK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178ec1b-b920-4f47-8243-82fd9f2754a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=\"index_endpoint_for_category\",\n",
    "    description=\"category index\",\n",
    "    network=VPC_NETWORK_FULL,\n",
    ")\n",
    "\n",
    "INDEX_ENDPOINT_NAME = my_index_endpoint.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d3346-2e4d-4332-9b7f-3129088b2981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "brute_force_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95315a-ea8f-4ff0-9409-63e14fab9611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_index_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262fdae9-1872-4292-b4d6-5380bbd51ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy\n",
    "DEPLOYED_BRUTE_FORCE_INDEX_ID = \"brute_force_deploy_comma_text_004_20holdout\"\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=brute_force_index, deployed_index_id=DEPLOYED_BRUTE_FORCE_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e99ac6-ca4b-41ce-b9f6-88a53f483aa4",
   "metadata": {},
   "source": [
    "### Query and get ranked results\n",
    "Query the deployed index to find nearest neighbor match.\n",
    "\n",
    "**Important:** The text embedding model may encounter errors when processing certain non-English or special characters. Please ensure data is cleansed of such characters or pre-process them appropriately to prevent issues during embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bd2c6-160a-4b18-8e5f-7ee941db033d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # GET MatchingEngineIndexEndpoint if exists\n",
    "# my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
    "#     \"projects/####/locations/us-central1/indexEndpoints/####\"\n",
    "# )\n",
    "\n",
    "# DEPLOYED_BRUTE_FORCE_INDEX_ID = \"brute_force_deploy_comma_text_004_20holdout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc94c6f-652b-4dfb-8ca9-d8b437ea9a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_holdout['combined_text'] = (df_data_holdout['supplier'].fillna('') + ', ' + \n",
    "                       df_data_holdout['short_description_en'].fillna('') + ', ' + \n",
    "                       df_data_holdout['supplier_description_en'].fillna('') + ', ' + \n",
    "                       df_data_holdout['long_description_en'].fillna(''))\n",
    "\n",
    "df_data_holdout = df_data_holdout.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b7559-6da7-4912-a436-9398ced90e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "row = 2 # hold out set 1 - 20   ########### @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854500d-5ea5-4eef-8dc0-e59801272735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = df_data_holdout[\"combined_text\"][row]\n",
    "\n",
    "unwanted_chars = [\"™\",\"®\",\"©\"]  # Add more characters to remove as needed\n",
    "for char in unwanted_chars:\n",
    "    query = query.replace(char, \"\")\n",
    "    \n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778a8f7-7000-4c0b-bf03-22e2626fb9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "\n",
    "# Set up Application Default Credentials (ADC)\n",
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e164f32-091b-463e-9458-c25ed77e50d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get online text embeddings for hold out set\n",
    "\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/us-central1/publishers/google/models/text-embedding-004:predict\"\n",
    "\n",
    "headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }\n",
    "\n",
    "request_body = str(\n",
    "        {\n",
    "          \"instances\": [\n",
    "            { \"content\": query}\n",
    "          ],\n",
    "        }\n",
    ")\n",
    "\n",
    "r = requests.post(url, data=request_body, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ca47f-215f-40d7-a06d-fdf721c1945a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2b856-5535-4a8b-82d5-369c2423eb06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_data = r.json()\n",
    "\n",
    "# Access the embeddings\n",
    "embeddings = response_data['predictions'][0]['embeddings']['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5bb5e-0e10-4569-97bf-86eb848f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "response = my_index_endpoint.match(\n",
    "    deployed_index_id=DEPLOYED_BRUTE_FORCE_INDEX_ID,\n",
    "    queries=[embeddings],\n",
    "    num_neighbors=5,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bd431-3d82-480c-a04c-4d6e9a126780",
   "metadata": {},
   "source": [
    "### Map Response back to Class\n",
    "\n",
    "Retrieve the original catyegory classes from the Vector Search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370ba3e-aa35-4d92-b809-678759423551",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data = []\n",
    "for neighbor in response[0]:  # Accessing the inner list\n",
    "    matched_id = neighbor.id\n",
    "    distance = neighbor.distance\n",
    "    matched_class = df[df[\"unique_id\"] == int(matched_id)][\"Tier_5_ID\"].iloc[0]\n",
    "    matched_data.append(\n",
    "        {\"ID\": matched_id, \"Tier_5_ID\": matched_class, \"Distance\": distance}\n",
    "    )\n",
    "matched_df = pd.DataFrame(matched_data)\n",
    "[\n",
    "    (item[\"Tier_5_ID\"], item[\"Distance\"])\n",
    "    for item in matched_df[[\"Tier_5_ID\", \"Distance\"]].to_dict(orient=\"records\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b632f71-c04f-412d-b768-b32463da226b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predicted Classes\n",
    "\n",
    "filtered_df = matched_df[matched_df[\"Distance\"] >= 0.7]\n",
    "predicted_class = filtered_df[\"Tier_5_ID\"].unique().tolist()\n",
    "\n",
    "# Ground Truth\n",
    "\n",
    "ground_truth_class = df_data_holdout.loc[row, \"structure_assignment_euhierarchy\"]\n",
    "\n",
    "print(\n",
    "    f\"The prediction classes: {predicted_class} \\nThe ground truth class is {ground_truth_class}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bbb41-e8fd-4f57-9074-cf7076592c63",
   "metadata": {},
   "source": [
    "### Considerations for Refining and Improving Results\n",
    "* **Threshold Selection:**  Choose an appropriate similarity threshold to define matches, balancing accuracy and the tolerance for errors. \n",
    "* **Integration:** Integrate with LLM's. \n",
    "* **Post-Processing:** Apply additional techniques and rules to refine the matching results further.\n",
    "* **Fine Tuning:** Fine Tune the Text Gecko Embeddings model to increase overall embedding task effectiveness.\n",
    "* **Input Formatting**: Optimize the input data for embedding generation by strategically selecting and combining features into human-readable sentences, potentially improving the quality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff007f7-3971-467e-9e85-66d5ff20867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051827a3-3375-4ea0-88ce-5404a11c09da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
